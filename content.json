{"posts":[{"title":"DETR","text":"概要説明 DETR (DEtection TRansformer) は、物体検出（object detection）タスクにおける新しいアプローチとして、2020年に Facebook AI（現 Meta AI）によって発表されたアルゴリズムです。 DETR は、従来の物体検出モデルとは異なり、トランスフォーマーベースのアーキテクチャを使用することで、検出と分類のプロセスを大幅に簡素化しています。 論文：https://arxiv.org/pdf/2005.12872 DETR の特徴 Transformer Architecture 自然言語処理で成果を挙げたTransformerを画像処理に応用しています。 入力画像をエンコーダーで特徴量に変換し、トランスフォーマーデコーダーが物体の位置（bbox）とClass labelを予測できます。 End-to-endのアプローチ 従来の物体検出では、アンカー生成や後処理（NMS: Non-Maximum Suppression）などの手作業のステップが必要でしたが、DETR ではこれらが不要となります。 これにより、シンプルで効率的な学習が可能になっています。 マルチタスク(物体検出と分類を同時に実行) DETR は、画像中の物体の位置を検出し、それらを分類するプロセスを統一されたフレームワーク内で行います。 環境の準備 DETR を動かすには、Python と PyTorch の環境が必要です。以下の手順で準備を進めます。 Python 環境の作成 12345# 仮想環境の作成（例: conda）conda create -n detr_env python=3.9 -yconda activate detr_envPyTorch のインストールPyTorch を CUDA 対応でインストールします（GPU 使用の場合）。 123456コードをコピーする# GPU 用 PyTorch (CUDA 11.7) のインストールpip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117# CPU 用 PyTorch のインストール（GPU がない場合）pip install torch torchvision torchaudio DETRインストール 早速、Git clone を実行して、確認しましょう。 12345git clone https://github.com/facebookresearch/detr.gitcd detr# 必要なライブラリをインストールpip install -r requirements.txt DETR を使用した推論 それでは、COCO データセットを使用した事前学習済みモデルで物体検出を行います。 detr_inference.py ファイルを作成して推論コードを準備します。 1234567891011121314151617181920212223242526import torchfrom transformers import DetrForObjectDetection, DetrImageProcessorfrom PIL import Imageimport requests# 事前学習済みモデルとプロセッサのロードprocessor = DetrImageProcessor.from_pretrained(&quot;facebook/detr-resnet-50&quot;)model = DetrForObjectDetection.from_pretrained(&quot;facebook/detr-resnet-50&quot;)# 入力画像を用意url = &quot;https://images.unsplash.com/photo-1593642532973-d31b6557fa68&quot;image = Image.open(requests.get(url, stream=True).raw)# 推論inputs = processor(images=image, return_tensors=&quot;pt&quot;)outputs = model(**inputs)# 物体検出結果を取得results = processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=[image.size])# 検出結果を出力for result in results: for score, label, box in zip(result[&quot;scores&quot;], result[&quot;labels&quot;], result[&quot;boxes&quot;]): print(f&quot;Label: {model.config.id2label[label.item()]}, Score: {score.item():.3f}, Box: {box.tolist()}&quot;) 保存して実行します。 1python detr_inference.py DETRのTraining 1python -m torch.distributed.launch --nproc_per_node=1 --use_env main.py --coco_path /workspaces/bev-playground/dataset/coco","link":"2024/12/19/DETR/"},{"title":"add-mermaind","text":"環境セットアップ hexo-filter-mermaid-diagramsのインストール 1npm install hexo-filter-mermaid-diagrams --save Mermaidの_config.ymlの設定 12345mermaid: enable: true version: \"7.1.2\" # Available themes: default | dark | forest | neutral theme: default Mermaidの読み込み hexo-theme-icarus/layout/common/scripts.jsxに以下を追加する 1&lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.4.0/mermaid.min.js\"&gt;&lt;/script&gt; ダイアグラム図作成 フローチャート図 12345graph TD; A--&gt;B; A--&gt;C; B--&gt;D; C--&gt;D; graph TD; A--&gt;B; A--&gt;C; B--&gt;D; C--&gt;D; click D \"https://google.com\"; シーケンス図 1234567891011sequenceDiagram participant Alice participant Bob Alice-&gt;&gt;John: Hello John, how are you? loop Healthcheck John-&gt;&gt;John: Fight against hypochondria end Note right of John: Rational thoughts &lt;br/&gt;prevail... John--&gt;&gt;Alice: Great! John-&gt;&gt;Bob: How about you? Bob--&gt;&gt;John: Jolly good! sequenceDiagram participant Alice participant Bob Alice-&gt;&gt;John: Hello John, how are you? loop Healthcheck John-&gt;&gt;John: Fight against hypochondria end Note right of John: Rational thoughts prevail... John--&gt;&gt;Alice: Great! John-&gt;&gt;Bob: How about you? Bob--&gt;&gt;John: Jolly good! ガンチャート図 123456789ganttdateFormat YYYY-MM-DDtitle Adding GANTT diagram to mermaidsection A sectionCompleted task :done, des1, 2014-01-06,2014-01-08Active task :active, des2, 2014-01-09, 3dFuture task : des3, after des2, 5dFuture task2 : des4, after des3, 5d gantt dateFormat YYYY-MM-DD title Adding GANTT diagram to mermaid section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d クラス図 1234567891011121314classDiagramClass01 &lt;|-- AveryLongClass : CoolClass03 *-- Class04Class05 o-- Class06Class07 .. Class08Class09 --&gt; C2 : Where am i?Class09 --* C3Class09 --|&gt; Class07Class07 : equals()Class07 : Object[] elementDataClass01 : size()Class01 : int chimpClass01 : int gorillaClass08 &lt;--&gt; C2: Cool label classDiagram Class01 &lt;|-- AveryLongClass : Cool Class03 *-- Class04 Class05 o-- Class06 Class07 .. Class08 Class09 --&gt; C2 : Where am i? Class09 --* C3 Class09 --|&gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla Class08 &lt;--&gt; C2: Cool label Gitグラフ 12345678910gitGraph commit commit branch develop commit commit commit checkout main commit commit gitGraph commit commit branch develop commit commit commit checkout main commit commit Quadrant Chart 123456789101112131415quadrantChart title Reach and engagement of campaigns x-axis Low Reach --&gt; High Reach y-axis Low Engagement --&gt; High Engagement quadrant-1 We should expand quadrant-2 Need to promote quadrant-3 Re-evaluate quadrant-4 May be improved Campaign A: [0.3, 0.6] Campaign B: [0.45, 0.23] Campaign C: [0.57, 0.69] Campaign D: [0.78, 0.34] Campaign E: [0.40, 0.34] Campaign F: [0.35, 0.78] quadrantChart title Reach and engagement of campaigns x-axis Low Reach --&gt; High Reach y-axis Low Engagement --&gt; High Engagement quadrant-1 We should expand quadrant-2 Need to promote quadrant-3 Re-evaluate quadrant-4 May be improved Campaign A: [0.3, 0.6] Campaign B: [0.45, 0.23] Campaign C: [0.57, 0.69] Campaign D: [0.78, 0.34] Campaign E: [0.40, 0.34] Campaign F: [0.35, 0.78] Pie chart diagrams 12345pie title Pets adopted by volunteers \"Dogs\" : 386 \"Cats\" : 85 \"Rats\" : 15 pie title Pets adopted by volunteers \"Dogs\" : 386 \"Cats\" : 85 \"Rats\" : 15 フローチャート flowchart A(開始) B(終了) 処理1 C[処理2] D{判断} E[処理2-2] F{{準備}} G1[/外部データ&lt;上にあると入力&gt;/] G2[/外部データ&lt;下にあると出力&gt;/] H[[定義&lt;マクロ&gt;]] I[(データベース)] J1((結合子A)) J2((結合子A')) K1[/ループ開始\\] K2[\\ループ終了条件式/] A --&gt; F F --&gt; K1 K1 --&gt; 処理1 処理1 --&gt; D D --&gt; E &amp; G1 E --&gt; H G1 --&gt; C --&gt; G2 G2 &amp; H --&gt; K2 --&gt; B H -.- J1 J2 --&gt; マクロ内容開始 --- I --- マクロ内容終了 --&gt; J2","link":"2024/12/27/add-mermaind/"},{"title":"bevformer","text":"概要説明","link":"2024/12/27/bevformer/"},{"title":"create-hexo-website","text":"Generate new article Create new post 1hexo new \"first post\" Create a new page on Hexo or Algorithm. 12hexo new page hexohexo new page algorithm Run Hexo server locally 1hexo server Generate the Website 12hexo cleanhexo generate Deploy your Website to Github Push source to public 1hexo deploy","link":"2024/12/27/create-hexo-website/"},{"title":"install-hexo","text":"Hexo インストール 123hexo init &lt;folder&gt;cd &lt;folder&gt;npm install hexo-theme-icarusのインストール 1npm install -S hexo-theme-icarus hexo-renderer-inferno _config.yml ファイル 1theme: icarus もしくは 1hexo config theme icarus","link":"2024/12/27/install-hexo/"},{"title":"BEVDet","text":"概要説明 マルチカメラの3D 既存技術より優れ キー技術 Image-view Encoder View Transformer","link":"2024/12/28/BEVDet/"},{"title":"CaDDN","text":"概要説明 Categorical Depth Distribution Network for Monocular 3D Object Detection Cody Reading Ali Harakeh Julia Chae Steven L. Waslander University of Toronto Robotics Institute https://arxiv.org/pdf/2103.01100","link":"2024/12/28/CaDDN/"},{"title":"basic-cnn-models","text":"Dataset 画像 MNIST ImageNet COCO2017 Cityscapes KITTI nuScenes Megaface WaymoOpen 音声 LibriSpeech AudioSet Common Voice Early models モデル 発表年 学会または発表場所 論文タイトル URL AlexNet 2012 NIPS 2012 (現NeurIPS) ImageNet Classification with Deep Convolutional Neural Networks AlexNet VGG16 2014 arXiv (未発表) Very Deep Convolutional Networks for Large-Scale Image Recognition VGG16 GoogLeNet 2014 CVPR 2015 (2014年発表) Going Deeper with Convolutions GoogLeNet ResNet 2015 CVPR 2016 (2015年発表) Deep Residual Learning for Image Recognition ResNet DenseNet 2016 CVPR 2017 (2016年発表) Densely Connected Convolutional Networks DenseNet Application models Detection 論文名 発表時間 発表者 発表組織 URL R-CNN 2013/11 Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik UC Berkeley link Fast R-CNN 2015/04 Ross Girshick Microsoft Research link Faster R-CNN 2015/06 Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun Microsoft Research link YOLO 2015/06 Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi University of Washington, Allen Institute for AI link SSD 2015/12 Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg Google Research, University of North Carolina, Chapel Hill link RetinaNet 2017/08 Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollár Facebook AI Research link YOLOv3 2018/4 Joseph Redmon, Ali Farhadi University of Washington, Allen Institute for AI link CenterNet 2019/05 Xingyi Zhou, Dequan Wang, Philipp Krähenbühl UT Austin link YOLOv4 2020/04 Alexey Bochkovskiy, Chien-Yao Wang, Hong-Yuan Mark Liao Independent &amp; Academia Sinica link YOLOv5 2020/10 Ultralytics Team Ultralytics link EfficientDet 2020/03 Mingxing Tan, Ruoming Pang, Quoc V. Le Google Research link DETR 2020/05 Nicolas Carion, Francisco Massa, Gabriel Synnaeve, et al. Facebook AI Research link Deformable DETR 2020/10 Xiaohang Zeng, Xizhou Zhu, Yue Cao, et al. Microsoft Research Asia link Segmentation 論文名 発表時間 発表者 発表組織 URL FCN 2014/11 Jonathan Long, Evan Shelhamer, Trevor Darrell UC Berkeley link U-Net 2015/05 Olaf Ronneberger, Philipp Fischer, Thomas Brox University of Freiburg link SegNet 2015/11 Vijay Badrinarayanan, Alex Kendall, Roberto Cipolla University of Cambridge link DeepLab 2016/06 Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L. Yuille Google DeepMind &amp; University of Maryland link PSPNet 2016/12 Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia Chinese University of Hong Kong link Mask R-CNN 2017/03 Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick Facebook AI Research link DeepLabv3 2017/09 Liang-Chieh Chen, George Papandreou, Florian Schroff, Hartwig Adam Google Research link Semantic FPN 2018/02 Xiaoxiao Li, Ross Girshick, Kaiming He, Piotr Dollár Facebook AI Research link DeepLabv3+ 2018/03 Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, Hartwig Adam Google Research link HRNet 2019/04 Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, et al. Microsoft Research Asia link DETR 2020/05 Nicolas Carion, Francisco Massa, et al. Facebook AI Research link ViT (Vision Transformer) 2020/06 Alexey Dosovitskiy, Lucas Beyer, et al. Google Research link PointRend 2020/03 Alexander Kirillov, Yuxin Wu, Kaiming He, Ross Girshick Facebook AI Research link Swin Transformer 2021/03 Ze Liu, Yutong Lin, Yue Cao, et al. Microsoft Research Asia link SegFormer 2021/06 Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo CUHK &amp; NVIDIA Research link Swin-UNet 2021/07 Hu Cao, Yue Cao, Zheng Zhang, Ming-Hsuan Yang, Ran He, Jian Yang Nanjing University of Science and Technology link MaskFormer 2021/10 Bowen Cheng, Alex Schwing, Alexander Kirillov Facebook AI Research link Segment Anything 2023/04 Alexander Kirillov, Eric Mintun, et al. Meta AI link","link":"2024/12/28/basic-cnn-models/"},{"title":"Basic Deep Learning","text":"Book list of Deep learning 2018年ごろに以下2冊が入門本として勉強しました。 深層学習(岡谷 貴之 著) 2015年 ゼロから作るDeep Learning - Pythonで学ぶディープラーニングの理論と実装(斎藤 康毅 著) 2016年 CNNの基本 Loss関数 Neural Networkは最適なパラメータ(Weightとbias)を見つけるため、学習で損失関数が最小値を取るとき 2乗和誤差 クロースエントロピー誤差 パラメータ更新 https://github.com/j-w-yun/optimizer-visualization SGD(確率的勾配降下法) Momentum 勾配の累積 (モメンタム更新): パラメータの更新: AdaGrad 勾配の累積二乗和: パラメータの更新: Adam 勾配の移動平均 (モーメント計算): バイアス補正: パラメータの更新: 正則化 モデルが過学習（オーバーフィッティング）するのを防ぐため目的にパラメータに何らかの制約を課すことです よく使われる正則化手法は以下です。 制約付き最適化(KKT条件から導く) L2正則化(Ridge回帰) 寄与が小さい重みを抑える L1正則化(Lasso回帰) 寄与が小さい重みをゼロにする Dropout データ拡張 Early Stopping バッチ正則化","link":"2024/12/28/basic-dl/"},{"title":"Math-Euclid","text":"人物 ユークリッド ( Euclid, 紀元前330年 - 紀元前275年頃) は古代ギリシアの数学者です。 『ユークリッド原論』の著者であり、「幾何学の父」と称されます。 実績 『ユークリッド原論』 1～4の公理・公準を述べている 第5公準「平行線公準」 基本概念 公理、定理、公準、命題 三角の内角は180度であるを証明する ユークリッド空間 位相空間 ユークリッド距離 ユークリッド内積 非ユークリッド幾何学 non-Euclidean geometry ユークリッド幾何学の平行線公準が成り立たないとして成立する幾何学である。 結論 楕円幾何学 ユークリッド幾何学（放物幾何学） 双曲幾何学 平行線の数 0本 1本 2本以上 代表的なモデル リーマン球面 ユークリッド平面 擬球面 Ivanovich Lobachevsky(ロシア数学者)：双曲幾何学 Georg Friedrich Bernhard Riemann(ドイツの数学者):楕円幾何学(リーマン球面) ユークリッドの互除法 Euclidean Algorithm 中学受験問題：689と1007の最大公約数はいくつですか？ ユークリッドの除法 Euclidean Division ユークリッドの定理 素数が無限であること ユークリッドの整域","link":"2024/12/29/Math-Euclid/"},{"title":"Math-Pythagoras","text":"ピタゴラス (Pythagoras, 紀元前570年頃-紀元前495年頃) は古代ギリシアの数学者です。 ピタゴラスの定理（直角三角形の三辺の関係を示す）はピタゴラスによる発見されていました。 そういえば、ピタゴラスの定理を証明する方法が、たくさんあると、中高学校でどこかでやったことがあります。 ここでまとめます。 相似による証明 三角比による証明 外接円を用いた証明 正方形を用いた証明 内接円を用いた証明 オイラーの公式を用いた証明 三角関数の微分公式を用いた証明 三角関数の不定積分を用いた証明 三角関数の加法定理を用いた証明 冪級数展開を用いた証明 回転行列を用いた証明 三角関数と双曲線関数を用いた証明","link":"2024/12/29/Math-Pythagoras/"}],"tags":[{"name":"BEV","slug":"BEV","link":"tags/BEV/"}],"categories":[{"name":"ML","slug":"ML","link":"categories/ML/"},{"name":"Hexo","slug":"Hexo","link":"categories/Hexo/"},{"name":"Math","slug":"Math","link":"categories/Math/"}],"pages":[{"title":"neural chip","text":"コンパイラ最適化 Layer Group 1から4、5から8、8から12のように分けることができます。 しかし、ここで重要なのは、これは実際には組み合わせの問題であるという点です。 つまり、多くの異なる分け方が可能です。 極端な例では、1から12をすべて1つのグループにする方法、または1から12を12個のグループに分け、各層を1つのグループにする方法があります。 もともとGPU上で一般的に行われていた方法は、各層を1つのグループに分ける方法でした。 これは、各層を計算した後にDDRにStoreします。それを統合して次の層の計算を行うというプロセスに基づいています。 その後、グループ化が終わったら、次のステップとしてPruning操作を行います。 この操作を通じて、効果が良くないことがわかった場合には、その部分を削除します。 次に、Layer Groupを単位として指令の再配置（TilingやSchedule、Sherlockの割り当てなど）を行います。指令の再配置について後ほど説明します。 これらの操作がすべて完了した後、正常にプロセスが進む場合、それは「コンパイル可能」であることを意味します。 しかし、グループ化の段階で必要なメモリサイズ(SRAMの容量よりはるか大きい)が大きすぎる場合、コンパイルできないことがあります。 そのため、CoreGenerator/Performance estimationを用いて予測を行うことで、最も処理時間が短いとなる効率的なグループ化を選定します。 Tiling の回数で探索する必要があります。 CodeGenの過程では：Tiling方向、Tiling回数、keep in SRAM Tilingのプロセスを制御する際には、反復計算の方向や回数、中間出力の結果をさらに計算に持ち込むかどうかといった、実行時に意思決定が必要な戦略を考慮する必要があります。 これらの問題を解決するために、動的計画法（Dynamic Programming）を用いる方法を採用しています。 動的計画法を使って、最適なNLPのレイヤー分割を探索します。 ただし、計算が爆発的に増加しないように、最大のレイヤー数を制限しています。 例えば、最大50層までに制限するとします。この50層というのが、現実的に許容可能な上限になります。 具体例として、100層のモデルがあると仮定します。 もし完全探索を行うと、非常に多くの試行回数が必要になりますが、 EP（例えばEvolutionary Programming）などの手法を使うことで、試行回数を約5000回程度に減らせます。 さらに、各グループ内で最大のOP数を制限すれば、試行回数をさらに減少させることも可能です。 しかし、これらの試行には、Tiling（タイル分割）、Schedule（スケジュール）、オーバーヘッドの削減といった部分は含まれていません。 そのため、さらに極端な簡略化や最適化を行い、明らかに利益を生まない部分を削除する作業が必要です。 たとえば、Layer Groupを切り替える場合、Proof、Style、その他の要素を1つのグループにまとめたと仮定します。 このとき、どのようにTilingを行うべきでしょうか？Tilingを行うことによって得られる利点は何でしょうか？ このような問いは、多くの研究者や開発者が直面している課題であり、さまざまな場面で議論されています。 Scheduling Schedulingの過程では、演算容量の状況を考慮する必要があります。 例えば、この場所で使用するクライアントソフトウェアがある場合、そのスケジュールがこの場所に割り当てられると、入力と出力の演算容量が収まらない可能性があります。 その場合、仕方なく後回しにする必要がある場合もあります。 したがって、スケジューリング時には演算容量の配分を考慮する必要があります。 当初、動的計画法DPを用いて、演算容量の配分は非常に簡単だと思われていました。例えば、等分する方法で調整する場合、各Tileのサイズを均一にすればよいと考えられていました。 しかし、現在ではモデルがますます複雑になり、このような状況が多様化しています。タイルのサイズもさまざまで、それに伴う状況も複雑になっています。 時には、適切に分割できないことで、ある場所には収まるが、別の場所には収まらないといった事態が発生します。その結果、演算容量の配分アルゴリズムの複雑さが大幅に増大しているのが現状です。 コンパイラモデル LEAP(DSL for Efficient AI Programming) 例として、Input → ProPrcess → NeuralNetwork → PostProcess → Output Torch Core : DSL: Triton: C/C++: 123456789# example inputexample_inputs = np.none((1,224,224,3), dtype='int8')# convert to mlirmlir_module = traslate(model_pipeline, example_inputs)# compilebayes_module = convert(mlir_module, \"bayes\")compile(bayes_module, output='xxx.hbm)","link":"achieve/2024_12_27_neural-chip.html"},{"title":"Algorithm","text":"Vision-base自動運転研究の遷移（抜粋） Flowchart flowchart LR subgraph 2017. A([Transformer ....]) click A \"/2024/12/19/DETR/\"; end subgraph 2020.. A --&gt; B([DETR ...]) click B \"/2024/12/19/DETR/\"; A --&gt; C([ViT ...]) B --&gt; D([Deformable-DETR .....]) L([LSS ..]) end subgraph 2021.. D --&gt; E([DETR3D ...]) C --&gt; F([SwinTransformer .....]) L --&gt; Ca([CaDDN ...]) Ca --&gt; Be([BEVDet ...]) end subgraph 2022.. E --&gt; G([BEVFormer ...]) G --&gt; H([ST-P3...]) Be --&gt; Be4([BEVDet4D ..]) L --&gt; BeFu([BEVFusion ..]) end subgraph 2023.. H --&gt; I([UniAD...]) G --&gt; I B --&gt; J([MapTR...]) J --&gt; K([VAD...]) I --&gt; K G --&gt; K end 2017 Transformer: Attention Is All You Need 点群ベース VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection PointNet PointNet++ Frustrum PointNets VoxelNet PointPillars Basic Models","link":"algorithm/index.html"},{"title":"hexo","text":"Installing Hexo Create Hexo website Adding Mermaind into Hexo","link":"hexo/index.html"},{"title":"math","text":"数学者名 (英語名) 国 生没 主な実績 Note ピタゴラス (Pythagoras) 古代ギリシャ BC570～495 ピタゴラスの定理、数と音楽の調和の研究 link ユークリッド (Euclid) 古代ギリシャ BC330～265 『原論』の執筆、幾何学の体系化 link アルキメデス (Archimedes) 古代ギリシャ BC287～212 浮力の原理、円周率の計算、てこの原理 アル＝フワーリズミ (Al-Khwarizmi) ペルシア 780～850 代数学の基礎、アルゴリズムの概念を提唱 レオナルド・フィボナッチ (Leonardo Fibonacci) イタリア 1170～1240 フィボナッチ数列、インド式数字の普及 ルネ・デカルト (René Descartes) フランス 1596～1650 座標幾何学の創設、デカルト座標系 ジョン・ネイピア (John Napier) スコットランド 1550～1617 対数の発明、ネイピア数の基礎 ジラール・デザルグ (Girard Desargues) フランス 1591～1661 射影幾何学の基礎、デザルグの定理 ピエール・ド・フェルマー (Pierre de Fermat) フランス 1607～1665 フェルマーの小定理、数論の発展 ブレーズ・パスカル (Blaise Pascal) フランス 1623～1662 確率論の基礎、パスカルの三角形 アイザック・ニュートン (Isaac Newton) イギリス 1643～1727 微分積分学の創設、万有引力の法則 ゴットフリート・ライプニッツ (Gottfried Wilhelm Leibniz) ドイツ 1646～1716 微分積分記法の確立、論理学の発展 ヤコブ・ベルヌーイ (Jacob Bernoulli) スイス 1654～1705 確率論と微分方程式の発展 ヨハン・ベルヌーイ (Johann Bernoulli) スイス 1667～1748 微分方程式、変分法の基礎 ブルック・テイラー (Brook Taylor) イギリス 1685～1731 テイラー展開、微積分学の発展 コリン・マクローリン (Colin Maclaurin) スコットランド 1698～1746 マクローリン級数、解析学への貢献 トーマス・ベイズ (Thomas Bayes) イギリス 1702年～1761年 ベイズの定理、確率論の基礎 レオンハルト・オイラー (Leonhard Euler) スイス 1707年～1783年 数学記号の標準化、オイラー公式、グラフ理論 ジョゼフ＝ルイ・ラグランジュ (Joseph-Louis Lagrange) フランス 1736～1813 ラグランジュ方程式、変分法 ピエール＝シモン・ラプラス (Pierre-Simon Laplace) フランス 1749～1827 ラプラス変換、確率論、天体力学 ジョセフ・フーリエ (Joseph Fourier) フランス 1768～1830 フーリエ解析、熱伝導方程式 カール・フリードリヒ・ガウス (Carl Friedrich Gauss) ドイツ 1777年～1855年 数論、ガウス平面、電磁気学への貢献 シメオン・ドニ・ポアソン (Siméon Denis Poisson) フランス 1781～1840 ポアソン分布、電磁気学 ジャン＝ヴィクトル・ポンスレ (Jean-Victor Poncelet) フランス 1788～1867 射影幾何学、ポンスレの定理 エヴァリスト・ガロア (Évariste Galois) フランス 1811年～1832年 群論の創設、代数方程式の解法の理論 ジョージ・ブール (George Boole) イギリス 1815年～1864年 ブール代数の発展、論理学と集合論の基礎 ベルンハルト・リーマン (Bernhard Riemann) ドイツ 1826年～1866年 リーマン幾何学、複素解析の発展 リヒャルト・デデキント (Richard Dedekind) ドイツ 1831～1916 実数の形式化、代数構造の理論 ソフス・リー (Sophus Lie) ノルウェー 1842～1899 リー群、リー代数 ウィリアム・ローワン・ハミルトン (William Rowan Hamilton) アイルランド 1805～1865 四元数、ハミルトン力学 カール・ワイエルシュトラス (Karl Weierstrass) ドイツ 1815～1897 厳密な解析学の基礎 ルートヴィヒ・シロー (Ludwig Sylow) ノルウェー 1832～1918 群論、シローの定理 ゲオルク・カントール (Georg Cantor) ドイツ 1845～1918 集合論の創設、無限の概念の体系化 アルフレッド・ノース・ホワイトヘッド (Alfred North Whitehead) イギリス 1861年～1947年 数理論理学の発展、『プリンキピア・マテマティカ』 ダフィット・ヒルベルト (David Hilbert) ドイツ 1862～1943 ヒルベルト空間、数学の形式化 アンリ・ルベーグ (Henri Lebesgue) フランス 1875～1941 ルベーグ積分、測度論の基礎 スリニヴァーサ・ラマヌジャン (Srinivasa Ramanujan) インド 1887～1920 数論、多項式恒等式、ラマヌジャン予想 ゴッドフレイ・ハロルド・ハーディ (Godfrey Harold Hardy) イギリス 1877～1947 数論、解析学、ラマヌジャンとの共同研究 ジョン・フォン・ノイマン (John von Neumann) ハンガリー/アメリカ 1903～1957 ゲーム理論、量子力学、コンピュータ科学 クルト・ゲーデル (Kurt Gödel) オーストリア 1906～1978 不完全性定理、数理論理学 アラン・チューリング (Alan Turing) イギリス 1912～1954 計算理論の基礎、チューリングマシン クロード・シャノン (Claude Shannon) アメリカ 1916～2001 情報理論の創設、シャノンのエントロピー","link":"math/index.html"}]}